# Observability Stack for RAG Admin
# ---------------------------------
# This file defines the SigNoz observability stack which provides:
# - Distributed tracing (see the full journey of each request)
# - Structured logging with trace correlation
# - Application metrics
#
# Usage:
#   Development: docker compose -f docker-compose.yml -f docker-compose.observability.yml up
#   Production:  docker compose -f docker-compose.prod.yml -f docker-compose.observability.yml up
#
# Access SigNoz UI at: http://localhost:3301

services:
  # ===========================================================================
  # ClickHouse - Time-Series Database
  # ===========================================================================
  # ClickHouse is a columnar database optimized for analytical queries.
  # It stores all observability data: traces, logs, and metrics.
  #
  # Why ClickHouse?
  # - Extremely fast for time-range queries (e.g., "show me errors in the last hour")
  # - Efficient compression for time-series data (saves disk space)
  # - SQL-like query language (familiar to most developers)
  #
  clickhouse:
    image: clickhouse/clickhouse-server:24.1-alpine
    container_name: rag-admin-clickhouse
    # Resource limits prevent ClickHouse from consuming all available memory
    # The "reservation" is what Docker guarantees; "limit" is the maximum allowed
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    volumes:
      # Persist data so it survives container restarts
      # Without this, you'd lose all historical observability data on restart
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
    # Health check ensures dependent services wait until ClickHouse is ready
    # The query "SELECT 1" is the simplest way to verify the database is accepting connections
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Restart on failure to maintain observability during transient issues
    restart: unless-stopped
    networks:
      - rag-network

  # ===========================================================================
  # SigNoz OTel Collector
  # ===========================================================================
  # The OpenTelemetry Collector acts as a telemetry pipeline:
  # 1. RECEIVES data from your application via OTLP (OpenTelemetry Protocol)
  # 2. PROCESSES it (batching, filtering, adding attributes)
  # 3. EXPORTS it to ClickHouse for storage
  #
  # Why use a Collector instead of sending directly to ClickHouse?
  # - Decouples your app from the storage backend (can switch backends easily)
  # - Handles batching and retries (your app doesn't wait for slow storage)
  # - Can filter/transform data before storage (reduce noise, add context)
  #
  signoz-otel-collector:
    image: signoz/signoz-otel-collector:0.88.25
    container_name: rag-admin-otel-collector
    # The collector is CPU-light but needs memory for buffering telemetry
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    # Command tells the collector where to find its configuration
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      # Mount our custom collector configuration
      - ./observability/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      # Port 4317: OTLP gRPC receiver
      # This is where your FastAPI app sends traces, logs, and metrics
      # gRPC is used because it's efficient for streaming telemetry data
      - "4317:4317"
      # Port 4318: OTLP HTTP receiver (alternative to gRPC)
      # Some clients prefer HTTP; provides the same functionality
      - "4318:4318"
      # Port 8888: Prometheus metrics about the collector itself
      # Useful for monitoring if the collector is healthy
      - "8888:8888"
    environment:
      # Tell the collector where ClickHouse is located
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=9000
    # Wait for ClickHouse to be healthy before starting
    # Otherwise, the collector would fail trying to export data
    depends_on:
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - rag-network

  # ===========================================================================
  # SigNoz Query Service
  # ===========================================================================
  # The Query Service provides:
  # - Web UI for exploring traces, logs, and metrics
  # - API for querying observability data
  # - Dashboard and alerting management
  #
  # This is what you interact with when debugging production issues.
  #
  signoz-query-service:
    image: signoz/query-service:0.88.25
    container_name: rag-admin-signoz
    deploy:
      resources:
        limits:
          memory: 768M
        reservations:
          memory: 512M
    environment:
      # ClickHouse connection settings
      - ClickHouseUrl=tcp://clickhouse:9000
      # Storage backend type
      - STORAGE=clickhouse
      # Disable Prometheus metrics (we use ClickHouse for everything)
      - PROMETHEUS_ENABLED=false
      # Where alerting rules are stored
      - ALERTMANAGER_API_PREFIX=http://localhost:9093/api/
    ports:
      # Port 3301: SigNoz Web UI
      # Open http://localhost:3301 in your browser to access dashboards
      - "3301:3301"
    depends_on:
      clickhouse:
        condition: service_healthy
      signoz-otel-collector:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3301/api/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - rag-network

# ===========================================================================
# Volumes - Persistent Storage
# ===========================================================================
# Docker volumes persist data outside the container filesystem.
# This means your observability data survives container restarts/updates.
#
volumes:
  clickhouse_data:
    name: rag-admin-clickhouse-data
  clickhouse_logs:
    name: rag-admin-clickhouse-logs

# ===========================================================================
# Networks
# ===========================================================================
# Using an external network allows observability containers to communicate
# with your application containers (defined in docker-compose.prod.yml).
# The "external: false" means Docker creates it if it doesn't exist.
#
networks:
  rag-network:
    name: rag-admin-network
    external: false
